{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Analytics Part 2 - Predictive Analytics: Building prediction models for continuous (regression) target variables\n",
    "\n",
    "In the previous part we have considered descriptive and exploratory analysis: understanding more about the data, its variables, and their relationships. Now, we turn our attention to so-called **Supervised Learning** methods. The goal of these supervised learning approaches is to model, or predict, a specific variable. Essentially, we want to answer the questions **\"What will happen?\"** and/or **\"What factors influence the predicted outcome?\"** (depending on if we focus on prediction or inference as the main goal of the analysis). \n",
    "\n",
    "<img src=\"Lifecycle.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the previous part we have considered descriptive and exploratory analysis: understanding more about the data, its variables, and their relationships. Now, we turn our attention to so-called **Supervised Learning** methods. The goal of these supervised learning approaches is to model, or predict, a specific variable.\n",
    "\n",
    "In general, the idea behind supervised learning is as follows: We have a target variable, $y$, and a set of potential predictor variables, $X$, that can help explain/predict the value of the target variable $y$. \n",
    "\n",
    "This can be summarized as follows:\n",
    "\n",
    "$$y = f(X) + \\epsilon$$\n",
    "\n",
    "where $\\epsilon$ is an error term that represents variation in the data/target variable $y$ that cannot be explained by the set of predictor variables (i.e., combined effect of variables that were not included in the equation).\n",
    "\n",
    "In general, we distinguish between two types of approaches: **regression**, and **classification**. \n",
    " - For regression models, $y$ is a continuous, numerical variable. Examples are: revenue, payments, cost, charges, customer spending, income, etc. \n",
    " - In contrast, for classification models y is a categorical variable, often binary: yes/no, 0/1, buy/not buy, churn/not churn, late return / no late return, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"SupervisedVsUnsupervised.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Training and  Evaluation - How do we know if the function $f(x)$ is the \"right\" one?\n",
    "\n",
    "Since we have different supervised learning models (i.e., functions $f(x)$) that we can use to model the relationship between the target variable $y$ and the predictor variables $x$, we need to introduce some basic evaluation steps for regression and classification models before we discuss specific models. Specifically, we want to get a measure of the **model performance**.\n",
    "\n",
    "For this, we consider two important questions:\n",
    "- How do I measure the \"goodness of fit\", i.e., quantify how close to the actual values for y the function $f(x)$ gets?\n",
    "- How do I make sure that these measures are generlizable, i.e., are representative of the model performance on new/unseen data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Evaluation - Performance Metrics\n",
    "\n",
    "Depending on the type of supervised learning problem, i.e., if we have a regression or classification problem, there are a set of standard evaluation metrics that we commonly consider:\n",
    "\n",
    "<img src=\"ModelEvaluationMetrics.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with some common metrics that measure how close the predictions $\\hat{y}$ are to the actual values $y$:\n",
    "\n",
    "- **Mean Squared Error (MSE)**\n",
    "A common measure of accuracy¬†is the mean squared error (MSE), i.e.,\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n}\\left( ùë¶_ùëñ‚àí \\hat{y}_ùëñ\\right)^2$$\n",
    "where $\\hat{y}_ùëñ$ is the prediction our method gives for the i-th observation in our training data.¬†\n",
    "\n",
    "- The **Root Mean Squared Error (RMSE)** simply is the square root of MSE:\n",
    "\n",
    "$$RMSE = \\sqrt{MSE}$$\n",
    "\n",
    "- Finally, the **Mean Absolute Error (MAE)** measure the absolute distance of predictions to actual values, instead of the squared distance:\n",
    "\n",
    "$$MAE = \\frac{1}{n} \\sum_{i=1}^{n}\\left| ùë¶_ùëñ‚àí \\hat{y}_ùëñ\\right|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- As an alternative measure, we can also express the percentage of variance in the target variable $y$ that is explained by the regression model $f(x)$ as the **coefficient of determination**, the $R^2$ metric:\n",
    "\n",
    "$$R^2 = \\frac{\\sum_{i=1}^{n} \\left( \\hat{y}_i - \\bar{y} \\right)^2 }{\\sum_{i=1}^{n} \\left(y_i - \\bar{y} \\right)^2} = \\frac{\\text{Sum of Squares Regression}}{\\text{Sum of Squares Total}}$$\n",
    "\n",
    "The $R^2$ metric measures how much variance in the target variable $y$ can be explained by the considered regression model. \n",
    "\n",
    "For example, an $R^2$ of 0.7 means that 70% of the variance in the target variable can be explained by the (here: linear) regression model. \n",
    "\n",
    "Generally, larger $R^2$ values mean that the regression model is able to explain a higher proportion of the variance in the target variable $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Evaluation and Validation\n",
    "\n",
    "Now that we know how to measure the general performance of a regression model, we also need to make sure that the performance measure doesn't just describe how well our regression model works on a given dataset, but is also generalizable to new/unseen data. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Model evaluation:</strong>\n",
    "One crucial part of Supervised Learning and Predictive Analytics is to get a realistic estimate of the expected performance of the created model. However, if we build the model on the entire dataset, i.e., use all available data to estimate the model parameters, we might experience <strong>overfitting</strong>: the phenomenon when your predictions work very well on the data used to train the model, but not on new, or unseen, data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Overfitting\"\n",
    "While it might be intuitive to use the entire data to train our supervised learning models, this approach will typically lead to **overfitting**, i.e., the following phenomenon: the fact that our models work well for the training data, i.e., the data it has already seen and is optimized for, but might not perform well on new, unseen, and slightly different data. This is particularly dangerous if we use very flexible functions f(x) (e.g., neural networks, non-linear regressions, etc.). \n",
    "\n",
    "<img src=\"Overfitting.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model performance estimation on \"unseen\" data\n",
    "To avoid overfitting and get a more realistic estimate of the estimated (generalization) performance of the models, we will adapt the way that we train/build and evaluate our models. Specifically, we will start with the simplest approach: splitting the dataset into separate subsets. One (the **training set**) will be used to estimate the model parameters, the other (the **test** set) will be used to estimate the model performance. The test set, thus, represents new/unseen data, i.e., data that has not been used to estimate the model parameters itself. Evaluating a model on data not used to train the model parameters is one way to get more realistic estimates of the model performance on new data.\n",
    "</div>\n",
    "\n",
    "<img src=\"TrainTest.png\" width=\"600px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While measuring the model performance (e.g., RMSE) on the separate test data is a much better approach to get generalizable model performance estimates, it is often only the first step in the right direction. Specifically, a single training-test split can suffer from a high variance in the estimates for model performance. In other words, if we repeat the training-test split with a different allocation of which observation falls in the training vs test set, the estimates for the model performance can show a rather high variance.\n",
    "\n",
    "This is why we often use extensions to the training-test split:\n",
    "- **Training-Validation-Test** split: If we have additional (hyper-)parameters in our models that we need to specify (e.g., the number of trees in a random forest, the number of nodes and layers in a neural network), selecting the parameters based on the test set performance might bias the results. Hence, the selection of good hyperparameters is often done on a separate validation set, and the model performance measured on the final test set.\n",
    "- **Cross-validation**: Here, we divide the data into k parts (folds). We fit k different models, using k-1 of the folds for model training and the remaining 1 fold for testing. The average model performance is then calculated as the average test set performance of the k models. This approach essentially yields a more stable (less variance) model performance estimate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>K-fold Cross validation:</strong>\n",
    "A single training-test split of the data is only a simple approach to avoid overfitting and has some disadvantages itself. Hence, in many Data Analytics projects, more advanced evaluation methods are used. The most commonly used process is <strong>k-fold cross validation</strong>. In this process, we split the data into k non-overlapping folds first. Then, we build k separate models, using k-1 of the folds as training set and the remaining fold as test set for the model. After building the k models and evaluating their performance on the respective test sets, we average the test set performance to get the average k-fold cross validation performance. This performance estimate tends to be even more accurate than using a single test set. \n",
    "</div>\n",
    "\n",
    "<img src=\"kfoldCV.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression Models\n",
    "\n",
    "Now that we know the basics of measuring model performance and how to train models in general, let's look at the first type of supervised learning models: **Regression models**. They consider the scenario where the target variable Y is continuous. There are many different types of regression models (functions f(x)) that we can consider, but here we start with a relatively simple one: Linear Regression.\n",
    "\n",
    "<img src=\"Regression.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Regression Model: Linear Regression\n",
    "\n",
    "Depending on the type of regression model that we want to use, a different function $f(X)$ is defined. The most common, and arguably most basic, version of this is the **Linear Regression** model: \n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_px_{ip} + \\epsilon_i, i = 1,\\ldots, n$$\n",
    "\n",
    "Here, each predictor variable $x$ affects the target variable $y$ independently, and linearly (hence, linear regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Least Squares Linear Regression:</strong>\n",
    "Linear regression is often estimated using the <strong>Least-squares</strong> procedure. In this procedure, we aim to find values for the $\\beta$ parameters such that we minimize the squared distance between actual and estimated values: \n",
    "    $$min \\sum_i (y_i - \\hat{y}_i)^2$$\n",
    "    \n",
    "However, this least squares procedure comes with a specific set of assumptions, such as \n",
    " - the error term $\\epsilon_i$ has conditional mean of zero\n",
    " - residuals follow a normal distribution, \n",
    " - independently and identically distributed data.\n",
    "\n",
    "We do not separately check for these assumptions here in this tutorial, but you are strongly encouraged to follow-up with (statistical) checks if these assumptions are met. More information on this can be found in all good statistics textbooks and/or material.\n",
    "</div>\n",
    "\n",
    "Let's create a linear regression model. Note that we first re-create the data preparation steps from the first session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for the library, we can also load the book reservations\n",
    "# here, they are tab-separated, so we need to specify the delimeter correctly\n",
    "lib_loan = pd.read_csv(\"../Data/LoanDataSample.csv\", sep=\",\", encoding='latin')\n",
    "\n",
    "lib_loan[\"Loan Date\"] = pd.to_datetime(lib_loan[\"Loan Date\"])\n",
    "lib_loan[\"Due Date\"] = pd.to_datetime(lib_loan[\"Due Date\"])\n",
    "lib_loan[\"Return Date\"] = pd.to_datetime(lib_loan[\"Return Date\"])\n",
    "\n",
    "# feature engineering: derive additional features\n",
    "lib_loan[\"Max Loan Time\"] = (lib_loan[\"Due Date\"] - lib_loan[\"Loan Date\"]).dt.days\n",
    "lib_loan[\"Actual Loan Time\"] = (lib_loan[\"Return Date\"] - lib_loan[\"Loan Date\"]).dt.days\n",
    "lib_loan[\"Late Return\"] = np.where((lib_loan[\"Return Date\"] - lib_loan[\"Due Date\"]).dt.days>0, 1, 0)\n",
    "lib_loan[\"Days Late\"] = np.where((lib_loan[\"Actual Loan Time\"] - lib_loan[\"Max Loan Time\"])>0, lib_loan[\"Actual Loan Time\"] - lib_loan[\"Max Loan Time\"], 0)\n",
    "\n",
    "# note: we also remove the spaces in the column names as they can create some issues later on\n",
    "lib_loan.columns = lib_loan.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, let's build a simple regression model that tries to predict the length of the loan. Here, we will split the available data into two parts: a training set, and a test set. We will estimate the $\\beta$ parameters on the training set and measure how well the model works (e.g., how close the estimates for the target variable are to the actual values) in the test set.\n",
    "\n",
    "Before we can build our model, we also additionally need to use a so-called encoder to convert categorical variables into variables that can be used by the regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# first, we create separate train and test subsets (splits)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# we drop the observations with missing values here\n",
    "# alternatives are various imputation methods that replace the missing values \n",
    "\n",
    "lib_loan_reg_data = lib_loan.loc[:, [\"UserGroup\", \"LibraryName\", \"LocationName\", \"MaxLoanTime\", \"ActualLoanTime\"]]\n",
    "lib_loan_reg_data.dropna(inplace=True)\n",
    "\n",
    "# then, we create dummy variables and split the data, using a randomly selected 80% of observations as training data, and 20%  as test\n",
    "lib_loan_reg_data = pd.get_dummies(lib_loan_reg_data)\n",
    "training, test = train_test_split(lib_loan_reg_data, test_size=.2, random_state = 22)\n",
    "\n",
    "X_train = training.loc[:,training.columns != \"ActualLoanTime\"]\n",
    "y_train = training[[\"ActualLoanTime\"]].values.ravel()\n",
    "\n",
    "X_test = test.loc[:,test.columns != \"ActualLoanTime\"]\n",
    "y_test = test[[\"ActualLoanTime\"]].values.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once we specified the training and test sets, we can **fit** (i.e., train) the model **on the training set** (and thus estimate the model parameters), and **evaluate** its performance **on the test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# let's build our first regression model, here a linear regression.\n",
    "# note that we estimate the model parameters on the training set and evaluate how well the predictions work on the test set.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# first, we estimate the model parameters on the training data (fitting the model)\n",
    "linear_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# then, we can let the model predict the outcome on the training and test sets\n",
    "linear_reg_pred_train = linear_reg.predict(X_train) # for the training data\n",
    "linear_reg_pred = linear_reg.predict(X_test) # for the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluate the performance\n",
    "\n",
    "Let's calculate these evaluation metrics for the training and test set. Note that for model comparison, i.e., to decide which type of regression model to use and/or which variables to include in the model, we need to use the performance on the test set, not the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"Mean Squared Error on the training set: \", mean_squared_error(y_train, linear_reg_pred_train))\n",
    "print(\"R2 Score on the training set: \", r2_score(y_train, linear_reg_pred_train))\n",
    "\n",
    "print(\"Mean Squared Error on the test set: \", mean_squared_error(y_test, linear_reg_pred))\n",
    "print(\"R2 Score on the test set: \", r2_score(y_test, linear_reg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that, as expected, we see that the model tends to perform better on the training data. However, as the test data represent new or unseen data, the performance on the test data is more realistic and should be used as the expected model performance instead.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong>Exercise 1:</strong>\n",
    "Re-run the previous regression model by using a different random number seed in the train_test_split function. I.e., instead of random_state=22, use a different integer. Calculate the training and test error rates. How do they compare to the initial estimates?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong>Exercise 2:</strong>\n",
    "Now, try a cross-validation setup. Go to the <a href=\"https://scikit-learn.org/stable/modules/cross_validation.html\">sklearn-documentation</a> and read up on how cross-validation works and is implemented in sklearn. Specifically, look at the function cross_val_score that uses your previous training-test split to further divide the training set into k folds, and evaluates the model based on the test set. \n",
    "    \n",
    "Calculate the mean cross-validation score (error rates, specifically the root-mean-squared-error). How does the mean score compare to the previous error estimate for the test set?</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression - Model and Interpretation\n",
    "\n",
    "**Note:** We will use standard Least Squares Regression (also called: Ordinary Least Squares (OLS) Regression) here. Depending on your data, alternative regression models and/or additional data preparation steps may be required (e.g., log-transforming the target variable, using robust regression instead of least-squares, etc.)!\n",
    "\n",
    "Here, we build a model to predict the actual loan time for a book. We consider the following key questions:\n",
    "\n",
    "1.  Which variables significantly affect how long a book is actually borrowed by a library customer?\n",
    "1.  Which variables decrease/increase the actual loan time?\n",
    "\n",
    "We can answer these questions in a linear regression model by considering the underlying statistical hypotheses for the linear regression model. Specifically, each independent variable (or dummy variable) will get its own $\\beta$ parameter estimate that measures the impact of this variable on the target variable (keeping all other variables constant). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Considering question 1, an OLS regression model looks at multiple statistical hypotheses, one for each $\\beta$ parameter. Specifically, it runs a statistical test if, based on the available data/evidence, the $\\beta$ parameter is actually different than 0 or not. If it is unlikely (given a specific Type I error threshold and the resulting p-value of the hypothesis test) that the $\\beta$ parameter is actually 0, we consider the corresponding variable to significantly affect the target variable (assuming the other model assumptions are correct). \n",
    "\n",
    "For the second question, the sign and magnitude of this $\\beta$ parameter estimate will tell us if this variable increases or decreases the estimate. \n",
    "\n",
    "Let's look at these questions for the model that predicts the actual loan time of a book. Specifially, we will be using [statsmodels](https://www.statsmodels.org/), a package that supports specifying and estimating statistical models using R-style formulas and pandas DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "training, test = train_test_split(lib_loan, test_size=.2, random_state = 22) # Note: train-test split is not really necessary if we are not interested in making predictions, but in explanatory statistics.\n",
    "\n",
    "# then, we specify that we want to use a standard \"Ordinary Least Squares (OLS)\" regression model\n",
    "linear_reg = smf.ols(formula=\"ActualLoanTime ~ UserGroup + LibraryName + MaxLoanTime\", data=training)    # Describe model\n",
    "\n",
    "# and estimate the parameters based on the available data X and y\n",
    "linear_reg_fit = linear_reg.fit()       # Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative version using dmatrices:\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "\n",
    "training, test = train_test_split(lib_loan, test_size=.2, random_state = 22)\n",
    "\n",
    "# first, we need to create design matrices and specify the target (dependent) and predictor (independent) variables\n",
    "y, X = dmatrices('ActualLoanTime ~ UserGroup + LibraryName + MaxLoanTime', data=training, return_type='dataframe')\n",
    "\n",
    "# then, we specify that we want to use a standard \"Ordinary Least Squares (OLS)\" regression model\n",
    "linear_reg = sm.OLS(y, X)    # Describe model\n",
    "\n",
    "# and estimate the parameters based on the available data X and y\n",
    "linear_reg_fit = linear_reg.fit()       # Fit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once we have fitted the model on the available (training) data, we can then look at the model coefficients and the p values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "linear_reg_fit.summary()   # Summarize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that for categorical predictor variables, by default the dmatrices will use one of the factor levels as a baseline and compare all other variables against it (by converting the categorical variable into **dummy variables**). Without additional specification, this typically uses a lexicographic ordering, i.e., uses the level that first appears in the alphabet first. Here, \"BibliothekarIn\" appears first in the lexicographic ordering. Hence, the $\\beta$ values of all other dummy variables generated for the UserGroup variable will measure the relative effect of the specific factor level compared to this baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "unique_levels = training[\"UserGroup\"].dropna().unique()\n",
    "sorted(unique_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here, we see several interesting results. First, variables with a small p-value are generally considered **significant**. From a statistical perspective, the underlying hypothesis test for individual significant is a t-test that tests the hypothesis if the true value of the $\\beta$ parameter of the specific predictor variable is 0 or not (under some assumptions for Least-Squares Linear Regression). In other words, p-values smaller than 0.05 (or 0.01, depending on the use case) are often considered evidence that this specific variable has an impact on the value of the target variable.\n",
    "\n",
    "In our case, some insights we can generate are:\n",
    "- `MaxLoanTime` is significant at the 0.001 level: Increasing MaxLoanTime by 1 unit increases the expected Actual Loan Time by 0.4461 units, given all other predictor variables are being held constant.\n",
    "-   Compared to the baseline UserGroup `BibliothekarIn`, the UserGroup `Campuslieferdienst` has a 63.0081 units higher estimated Actual Loan Time, given all other variables are being held constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong>Exercise 3:</strong> SK-learn also supports a wide range of <a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model\">linear models </a>, including <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\">Ordinary least squares Linear Regression</a>. Statsmodels also supports a wide rante of <a href=\"https://www.statsmodels.org/stable/regression.html\">Linear regression</a> models as well. However, you will notice that although both libraries can easily be used to fit linear models (and make predictions based on these fitted models), their design philosophy is quite different (statsmodels is a statistics package whereas sk-learn is a machine learning library). \n",
    "    \n",
    " - Compare and contrast the linear regressions we did with these two libraries above. \n",
    " - What do you notice? What are the major differences?\n",
    " - Which one would you prefer for what purpose?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alternative Regression Models\n",
    "\n",
    "Basic Least-Squares Linear Regression models are widely used due to their simplicity and easy interpretability. Of course, this is only one example of regression models, other examples include:\n",
    "\n",
    "-   Regularized Regression, e.g., Ridge Regression or LASSO\n",
    "-   Non-linear Regression, e.g., polynomial regression models, Splines, or Generalized Additive Models (GAMs)\n",
    "-   Tree-based models, e.g., Regression Trees, Random Forests\n",
    "-   Neural Network-based models\n",
    "-   Non-parametric models such as k-Nearest-Neighbor (kNN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Nearest Neighbor Model \n",
    "Let's look at a very different, very intuitive and flexible alternative: **k-nearest-neighbor (kNN)**.\n",
    "\n",
    "The idea behind kNN is fairly straightforward: \n",
    "-  Select a value for $k$. For example, 3 or 5\n",
    "-  For each observation, identify the $k$ closest observations, i.e., the most similar observations. For example, based on a Euclidean or similar distance metric, we identify the $k$ closest observations.\n",
    "-  Observe the value of the target variable for these k most similar observations.\n",
    "    -  For Regression: Average the target variable values for the k closest observations as estimate $\\hat{y} = \\frac{\\sum_{i \\in N_0} y_i}{K}$, where $N_0$ is the set of k closest neighbors.\n",
    "    -  For Classification: Take the majority outcome of the target variable as estimate \n",
    "    $\\hat{y} = Pr(Y=m | X=x_0) = \\frac{1}{K}\\sum_{i \\in N_0} I(y_i = m)$, \n",
    "    where $x_0$ is the observation which we want to predict and $m$ is one of the potential values of the target variable $y$.\n",
    "\n",
    "In general, the smaller the value of k, the more flexible the prediction will be. \n",
    "However, generally smaller values of k also increase the risk of overfitting.\n",
    "\n",
    "Let's build a kNN regression model for the lib_loan dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "knn_reg = knn.fit(X_train, y_train)\n",
    "\n",
    "knn_reg_pred_train = knn_reg.predict(X_train)\n",
    "knn_reg_pred = knn_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "print(\"kNN Mean Squared Error on the training set: \", mean_squared_error(y_train, knn_reg_pred_train))\n",
    "print(\"kNN R2 Score on the training set: \", r2_score(y_train, knn_reg_pred_train))\n",
    "\n",
    "print(\"kNN Mean Squared Error on the test set: \", mean_squared_error(y_test, knn_reg_pred))\n",
    "print(\"kNN R2 Score on the test set: \", r2_score(y_test, knn_reg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "While kNN is very flexible and only requires the selection of a value for k, it does have several potential downsides. In particular, kNN models suffer from the so-called **curse of dimensionality**: if we have many predictor variables in the dataset (i.e., include many predictor variables in our regression model), the performance of the kNN approach tends to deteriorate quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong>Exercise 4:</strong>\n",
    "Re-run the previous kNN with a smaller or larger number of k.     \n",
    "How do these models with different parameter values for k compare to the previous one? Compare the prediction performance. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LASSO: A (linear) model with integrated feature selection\n",
    "\n",
    "As we discussed in Part 1, we often have many variables in our data, of which not all are going to be useful for our prediction task. Hence, to avoid unnecessarily complex models, we aim to only include variables that are helpful for our prediction tasks. \n",
    "\n",
    "We already noted that there are several options for **variable selection** that we can use here: filter methods, wrapper methods, or embedded methods. Here, we are going to consider an alternative to Least-Squares Regression that falls under the embedded method category: **LASSO**.\n",
    "\n",
    "LASSO stands for **Least Absolute Shrinkage and Selection Operator**. Instead of minimizing the sum-of-squares as regular least-squares regression does, LASSO adds a so-called **regularization** term to the objective function that we try to minimize when finding the $\\beta$ parameters in our linear regression model:\n",
    "\n",
    "$$min \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What happens here? Basically, the second term $\\lambda \\sum_{j=1}^{p} \\beta_j^2$ adds a penaly to large $\\beta$ coefficients in the model. Intuitively, if we want to minimize this objective function, we can reduce the penalty by reducing (or setting to 0) the $\\beta$ coefficients of unnecessary predictors in the model, i.e., variables that do not improve the model performance. \n",
    "\n",
    "In fact, LASSO will set some of the $\\beta$ coefficients of variables to 0, which effectively removes them from the prediction model. Hence, it performs variable selection while estimating the values for the $\\beta$ coefficients.\n",
    "\n",
    "Let's see how we can build this LASSO model and which variables are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's build a lasso model\n",
    "# note that we estimate the model parameters on the training set and evaluate how well the predictions work on the test set.\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# first, we estimate the model parameters on the training data (fitting the model)\n",
    "lasso_reg = Lasso().fit(X_train, y_train)\n",
    "\n",
    "# then, we can let the model predict the outcome on the training and test sets\n",
    "lasso_reg_pred_train = lasso_reg.predict(X_train) # for the training data\n",
    "lasso_reg_pred = lasso_reg.predict(X_test) # for the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong>Exercise 5:</strong>\n",
    "Calculate the LASSO model performance on the training and test set, similar to how we calculated it for the linear regression model. Specifically, calculate the $R^2$ and the mean squared error.  \n",
    "    \n",
    "How do the training and test set performance metrics for the LASSO model compare against the previous linear regression model?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong>Exercise 6:</strong>\n",
    "Now, you are going to investigate the variable selection properties of the created LASSO model. Specifically, look up the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso\">LASSO documentation</a> to see how you can use the model coefficients (the $\\beta$ coefficients) to investigate which variables $j$ are removed from the model ($\\beta_j$=0).\n",
    "    \n",
    "- Print out the $\\beta$ coefficients of the LASSO model.\n",
    "- How many variables are included in the model, and how many are removed ($\\beta_j$=0)?\n",
    "- Do you see a trade-off between model complexity (number of included variables) and model performance (e.g., measured by MSE)?</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook: Non-linear Regression, Variable Selection\n",
    "\n",
    "A Linear Regression model is often the first step in a regression analysis, which may be followed by:\n",
    "\n",
    "1.  Trying non-linear regression models, generalized additive models (GAMs), or Splines as more flexible regression models.\n",
    "\n",
    "2.  Running alternative variable selection methods to determine which subset of predictor variables are most relevant and useful for the given model."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "2981dce05ec140a774f3ef0a6ec344bf2927c028c8e9c6b64295edbe14873f3d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
